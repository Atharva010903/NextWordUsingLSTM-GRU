# -*- coding: utf-8 -*-
"""Lstm_RNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QuwUiQEllNt7WXjnUUm3mVAIQQ6hAevT
"""

import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as pt

pip install streamlit

import streamlit as st

import tensorboard

pip install scikeras

import scikeras

import nltk
nltk.download('gutenberg')
from nltk.corpus import gutenberg

data=gutenberg.raw('shakespeare-hamlet.txt')
with open('shakespeare.txt','w') as file:
    file.write(data)

import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

with(open('shakespeare.txt','r')) as file:
    text=file.read()

tokenizer=Tokenizer()
tokenizer.fit_on_texts([text])
total_words=len(tokenizer.word_index)+1
total_words

input_sequences=[]
for line in text.split('\n'):
  token_list=tokenizer.texts_to_sequences([line])[0]
  for i in range(1,len(token_list)):
    n_gram_sequence=token_list[:i+1]
    input_sequences.append(n_gram_sequence)

input_sequences

max_sequence_length=max([len(x) for x in input_sequences])
max_sequence_length

input_sequences=np.array(pad_sequences(input_sequences,maxlen=max_sequence_length,padding='pre'))
input_sequences

x,y=input_sequences[:,:-1],input_sequences[:,-1]

y

y = tf.keras.utils.to_categorical(y, num_classes=total_words)
y

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)

x_train.shape

y_train.shape

x_train = x_train[:1000]  # Trim x_train to match y_train size

from tensorflow.keras.callbacks import EarlyStopping
early_stopping=EarlyStopping(monitor='val_loss',patience=3,restore_best_weights=True)

total_words

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, Dropout, Input
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping
import numpy as np

# Simulated dataset
corpus = [
    "to be or not to be",
    "the quick brown fox jumps over the lazy dog",
    "this is a test sentence",
    "deep learning is amazing",
    "the world is beautiful and vast",
    "AI is the future of technology"
]

# Tokenization with limited vocabulary
tokenizer = Tokenizer(num_words=2000, oov_token="<OOV>")  # Limiting vocab size
tokenizer.fit_on_texts(corpus)
total_words = len(tokenizer.word_index) + 1  # Update vocabulary size

# Generating input sequences
input_sequences = []
for line in corpus:
    token_list = tokenizer.texts_to_sequences([line])[0]
    for i in range(1, len(token_list)):
        n_gram_sequence = token_list[:i + 1]
        input_sequences.append(n_gram_sequence)

# Padding sequences
max_sequence_length = max([len(seq) for seq in input_sequences])
input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')

# Splitting into X and y
X, y = input_sequences[:, :-1], input_sequences[:, -1]
y = to_categorical(y, num_classes=total_words)

# Train-test split
split_ratio = 0.2
split_index = int(len(X) * (1 - split_ratio))
x_train, x_test = X[:split_index], X[split_index:]
y_train, y_test = y[:split_index], y[split_index:]

# Early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Function to create models
def create_model(use_gru=False):
    model = Sequential()
    model.add(Input(shape=(max_sequence_length - 1,)))
    model.add(Embedding(input_dim=total_words, output_dim=50))  # Reduced embedding size
    if use_gru:
        model.add(GRU(128, return_sequences=True))  # Reduced units
    else:
        model.add(LSTM(128, return_sequences=True))
    model.add(Dropout(0.3))
    model.add(LSTM(64))
    model.add(Dense(total_words, activation="softmax"))
    model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])
    return model

# Train LSTM model
lstm_model = create_model()
lstm_history = lstm_model.fit(x_train, y_train, epochs=50, validation_data=(x_test, y_test), verbose=1, callbacks=[early_stopping])

# Train LSTM-GRU model
gru_model = create_model(use_gru=True)
gru_history = gru_model.fit(x_train, y_train, epochs=50, validation_data=(x_test, y_test), verbose=1, callbacks=[early_stopping])

# Compare models
lstm_acc = max(lstm_history.history['val_accuracy'])
gru_acc = max(gru_history.history['val_accuracy'])

best_model = lstm_model if lstm_acc > gru_acc else gru_model
print(f"Best model: {'LSTM' if lstm_acc > gru_acc else 'LSTM-GRU'} with accuracy: {max(lstm_acc, gru_acc)}")

# Save best model
best_model.save("next_word_model.h5")

# Function to predict the next word
def predict_next_word(model, tokenizer, text, max_sequence_length):
    token_list = tokenizer.texts_to_sequences([text])[0]
    if len(token_list) >= max_sequence_length:
        token_list = token_list[-(max_sequence_length-1):]  # Ensure the sequence length matches max_sequence_len-1
    token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')
    predicted = model.predict(token_list, verbose=0)
    predicted_word_index = np.argmax(predicted, axis=1)
    for word, index in tokenizer.word_index.items():
        if index == predicted_word_index:
            return word
    return None

input_text="Barn. Last night of all,When yond same"
print(f"Input text: {input_text}")
max_sequence_length=best_model.input_shape[1]
predicted_word=predict_next_word(best_model,tokenizer,input_text,max_sequence_length)
print(f"Predicted word: {predicted_word}")

## Save the model
best_model.save("next_word_lstm.h5")
## Save the tokenizer
import pickle
with open('tokenizer.pickle','wb') as handle:
    pickle.dump(tokenizer,handle,protocol=pickle.HIGHEST_PROTOCOL)

